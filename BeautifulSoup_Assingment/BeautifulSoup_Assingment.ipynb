{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d54a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q:- 1) Write a python program to display IMDB’s Top rated 100 Indian movies’ data\n",
    "#       https://www.imdb.com/list/ls056092300/ (i.e. name, rating, year ofrelease) and make data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fbb0c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3cf5df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [403]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = requests.get('https://www.imdb.com/list/ls056092300/')\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "740fa4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html>\n",
       "<head><title>403 Forbidden</title></head>\n",
       "<body>\n",
       "<center><h1>403 Forbidden</h1></center>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(page.content)\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8399dac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2:- Write a python program to scrape details of all the posts from https://www.patreon.com/coreyms .Scrape the\n",
    "#heading, date, content and the likes for the video from the link for the youtube video from the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae50247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to scrape Patreon posts\n",
    "def scrape_patreon_posts(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    posts = soup.find_all('div', class_='post')\n",
    "\n",
    "    for post in posts:\n",
    "        heading = post.find('h2').text.strip()\n",
    "        date = post.find('time')['datetime']\n",
    "        content = post.find('div', class_='content').text.strip()\n",
    "        likes = post.find('span', class_='likes').text.strip()\n",
    "        youtube_link = post.find('a', href=True, text='YouTube')\n",
    "\n",
    "        print(f\"Heading: {heading}\")\n",
    "        print(f\"Date: {date}\")\n",
    "        print(f\"Content: {content}\")\n",
    "        print(f\"Likes: {likes}\")\n",
    "        if youtube_link:\n",
    "            print(f\"YouTube Link: {youtube_link['href']}\")\n",
    "        print('-' * 40)\n",
    "\n",
    "# URL of the Patreon page\n",
    "url = 'https://www.patreon.com/coreyms'\n",
    "scrape_patreon_posts(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67019f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a python program to scrape house details from mentioned URL. It should include house title, location,\n",
    "#area, EMI and price from https://www.nobroker.in/ .Enter three localities which are Indira Nagar, Jayanagar,\n",
    "#Rajaji Nagar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54948c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping details for Indira Nagar...\n",
      "\n",
      "Scraping details for Jayanagar...\n",
      "\n",
      "Scraping details for Rajaji Nagar...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to scrape house details from NoBroker\n",
    "def scrape_nobroker(locality):\n",
    "    url = f'https://www.nobroker.in/property/sale/{locality.replace(\" \", \"-\").lower()}_bangalore'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    houses = soup.find_all('div', class_='card')\n",
    "\n",
    "    for house in houses:\n",
    "        title = house.find('h2', class_='heading-6').text.strip()\n",
    "        location = house.find('div', class_='nb__2CMjv').text.strip()\n",
    "        area = house.find('div', class_='nb__3oNyC').text.strip()\n",
    "        price = house.find('div', class_='nb__3qQ9m').text.strip()\n",
    "        emi = house.find('div', class_='nb__1iA8a').text.strip()\n",
    "\n",
    "        print(f\"Title: {title}\")\n",
    "        print(f\"Location: {location}\")\n",
    "        print(f\"Area: {area}\")\n",
    "        print(f\"Price: {price}\")\n",
    "        print(f\"EMI: {emi}\")\n",
    "        print('-' * 40)\n",
    "\n",
    "# Localities to scrape\n",
    "localities = ['Indira Nagar', 'Jayanagar', 'Rajaji Nagar']\n",
    "\n",
    "for locality in localities:\n",
    "    print(f\"Scraping details for {locality}...\\n\")\n",
    "    scrape_nobroker(locality)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede7f399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python program to scrape first 10 product details which include product name , price , Image URL from\n",
    "#https://www.bewakoof.com/bestseller?sort=popular ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d396df1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# URL of the Bewakoof bestseller page\u001b[39;00m\n\u001b[0;32m     22\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.bewakoof.com/bestseller?sort=popular\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 23\u001b[0m scrape_bewakoof_bestsellers(url)\n",
      "Cell \u001b[1;32mIn[38], line 13\u001b[0m, in \u001b[0;36mscrape_bewakoof_bestsellers\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m product \u001b[38;5;129;01min\u001b[39;00m products:\n\u001b[0;32m     12\u001b[0m     name \u001b[38;5;241m=\u001b[39m product\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh3\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m---> 13\u001b[0m     price \u001b[38;5;241m=\u001b[39m product\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiscountedPriceText\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     14\u001b[0m     image_url \u001b[38;5;241m=\u001b[39m product\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproductImgTag\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProduct Name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to scrape product details from Bewakoof\n",
    "def scrape_bewakoof_bestsellers(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    products = soup.find_all('div', class_='productCardBox', limit=10)\n",
    "\n",
    "    for product in products:\n",
    "        name = product.find('h3').text.strip()\n",
    "        price = product.find('span', class_='discountedPriceText').text.strip()\n",
    "        image_url = product.find('img', class_='productImgTag')['src']\n",
    "\n",
    "        print(f\"Product Name: {name}\")\n",
    "        print(f\"Price: {price}\")\n",
    "        print(f\"Image URL: {image_url}\")\n",
    "        print('-' * 40)\n",
    "\n",
    "# URL of the Bewakoof bestseller page\n",
    "url = 'https://www.bewakoof.com/bestseller?sort=popular'\n",
    "scrape_bewakoof_bestsellers(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac22720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please visit https://www.cnbc.com/world/?region=world and scrap-\n",
    " #a) headings\n",
    " #b) date\n",
    " #c) News link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f207165",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# URL of the CNBC world news page\u001b[39;00m\n\u001b[0;32m     22\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.cnbc.com/world/?region=world\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 23\u001b[0m scrape_cnbc_news(url)\n",
      "Cell \u001b[1;32mIn[39], line 13\u001b[0m, in \u001b[0;36mscrape_cnbc_news\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m articles:\n\u001b[0;32m     12\u001b[0m     heading \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m---> 13\u001b[0m     date \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     14\u001b[0m     news_link \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHeading: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheading\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to scrape news details from CNBC\n",
    "def scrape_cnbc_news(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    articles = soup.find_all('div', class_='Card-titleContainer')\n",
    "\n",
    "    for article in articles:\n",
    "        heading = article.find('a').text.strip()\n",
    "        date = article.find('time')['datetime']\n",
    "        news_link = article.find('a')['href']\n",
    "\n",
    "        print(f\"Heading: {heading}\")\n",
    "        print(f\"Date: {date}\")\n",
    "        print(f\"News Link: {news_link}\")\n",
    "        print('-' * 40)\n",
    "\n",
    "# URL of the CNBC world news page\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "scrape_cnbc_news(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0782d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6) Please visit https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloadedarticles/ and scrapa) Paper title\n",
    " # b) date\n",
    " #c) Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "efb8307c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to scrape most downloaded articles from KeAi Publishing\n",
    "def scrape_keaipublishing(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    articles = soup.find_all('div', class_='article-content')\n",
    "\n",
    "    for article in articles:\n",
    "        title = article.find('h2').text.strip()\n",
    "        date = article.find('span', class_='date').text.strip()\n",
    "        authors = article.find('span', class_='authors').text.strip()\n",
    "\n",
    "        print(f\"Paper Title: {title}\")\n",
    "        print(f\"Date: {date}\")\n",
    "        print(f\"Authors: {authors}\")\n",
    "        print('-' * 40)\n",
    "\n",
    "# URL of the KeAi Publishing most downloaded articles page\n",
    "url = 'https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded-articles/'\n",
    "scrape_keaipublishing(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5330bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
